{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Notebook for simple text classification\n",
    "\n",
    "This notebooks contains code for simple classification of tweets into 'offensive' ('OFF') and non-offensive ('NOT'). \n",
    "\n",
    "You can use **simple vocabulary-counts as features** by calling:\n",
    "\n",
    "\n",
    "`path_to_data = 'path/to/your/offenseval_data/` (e.g. '../../../../../Data/offenseval/offenseval2017/')\n",
    "\n",
    "`classify_count(path_to_data)`\n",
    "\n",
    "Or you can use **embeddings as features** by calling:\n",
    "\n",
    "`path_to_model = path/to/embedding_model.bin' (e.g. ../../../../../Data/dsm/word2vec/GoogleNews-vectors-negative300.bin')`\n",
    "\n",
    "`path_to_data = 'path/to/your/offenseval_data/` (e.g. '../../../../../Data/offenseval/offenseval2017/')\n",
    "\n",
    "`model_name = 'google_news'` (give your model a name)\n",
    "\n",
    "`classify_embeddings(path_to_data, path_to_model, model_name)`\n",
    "\n",
    "The resulting predictions will be stored in the directory ./predictions/ and the performance (f1, recall, precision) will be printed to the screen. \n",
    "\n",
    "Note that you have to run all the cells in the notebook before you can call the functions as shown above. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definitions\n",
    "\n",
    "Run these cells.\n",
    "\n",
    "(Feel free to modify the code.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def load_data(data_dir, setname):\n",
    "    test_path = 'offenseval-trial.txt'\n",
    "    train_path = 'offenseval-training-v1.tsv'\n",
    "    if setname == 'test':\n",
    "        filepath = data_dir+test_path\n",
    "        data = pd.read_csv(filepath, \n",
    "                       delimiter = '\\t', \n",
    "                       header = None,  \n",
    "                       names=[\"tweet\", \"subtask_a\", \"subtask_b\", \"subtask_c\"])\n",
    "    elif setname == 'train':\n",
    "        filepath = data_dir+train_path\n",
    "        data = pd.read_csv(filepath, delimiter=\"\\t\")  \n",
    "    return data\n",
    "\n",
    "def split_train(train_data):\n",
    "    # split 90%, 10%\n",
    "    total = len(train_data)\n",
    "    total_90 = round(total * 0.9)\n",
    "    train_data_split = train_data[:total_90]\n",
    "    validation_data = train_data[total_90:]\n",
    "    return train_data_split, validation_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "from nltk import TweetTokenizer\n",
    "import string \n",
    "# tokenize, remove stop-words\n",
    "\n",
    "def tokenize(data, remove_stop_words = True):\n",
    "    tokenized_tweets = []\n",
    "    tokenizer = TweetTokenizer()\n",
    "    to_remove = list(string.punctuation)\n",
    "    to_remove.extend(['@USER', 'URL'])\n",
    "    for tweet in data['tweet']:\n",
    "        tokenized_tweet = ' '.join(tokenizer.tokenize(tweet))\n",
    "        if remove_stop_words == True:\n",
    "            for char in to_remove:\n",
    "                tokenized_tweet = tokenized_tweet.replace(char.strip(), '').lower()\n",
    "        tokenized_tweets.append(tokenized_tweet)\n",
    "    data['tweet_tok'] = tokenized_tweets\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform preprocessed tweets to vector representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainsform tweets to vocab count vectors \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def tweets_to_count_vec(tweets_train, tweets_test):\n",
    "    vectorizer = CountVectorizer()\n",
    "    train_X = vectorizer.fit_transform(tweets_train)\n",
    "    test_X = vectorizer.transform(tweets_test)\n",
    "    return train_X, test_X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform to embedding vecs (assuming gensim compatible model)\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "\n",
    "def tweets_to_embedding(tweets, model_path):\n",
    "    model = KeyedVectors.load_word2vec_format(model_path, binary = True)\n",
    "    data_X = []\n",
    "    for tweet in tweets:\n",
    "        #tweet was tokenized and joined by ' ' in the previous step\n",
    "        tokens = tweet.split(' ')\n",
    "        tweet_vecs = np.array([model[t] for t in tokens if t in model.vocab])\n",
    "        if len(tweet_vecs) > 1:\n",
    "            average_embedding = np.mean(tweet_vecs, axis = 0)\n",
    "        else:\n",
    "            n_d = len(model['the'])\n",
    "            average_embedding = np.zeros(n_d)\n",
    "        data_X.append(average_embedding)\n",
    "    return np.array(data_X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify tweets using an SVM binary classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify\n",
    "from sklearn import svm\n",
    "\n",
    "def train(train_X, train_y):\n",
    "    clf = svm.SVC(gamma='scale')\n",
    "    clf.fit(train_X, train_y)  \n",
    "    return clf\n",
    "\n",
    "def predict(clf, test_X):\n",
    "    predictions = clf.predict(test_X)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write predictions to a file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write output to file for further analysis\n",
    "import os \n",
    "import csv\n",
    "\n",
    "def predictions_to_file(tweets, gold, predictions, name):\n",
    "    \n",
    "    if not os.path.isdir('predictions/'):\n",
    "        os.mkdir('predictions/')\n",
    "    results_dict_list = []\n",
    "    \n",
    "    for tweet, gl, pl in zip(tweets, gold, predictions):\n",
    "        results_dict = dict()\n",
    "        results_dict['tweet'] = tweet\n",
    "        results_dict['gold_label'] = gl\n",
    "        results_dict['predicted_label'] = pl\n",
    "        results_dict_list.append(results_dict)\n",
    "    \n",
    "    with open(f'predictions/{name}.csv', 'w') as outfile:\n",
    "        writer = csv.DictWriter(outfile, fieldnames = results_dict_list[0].keys())\n",
    "        writer.writeheader()\n",
    "        for d in results_dict_list:\n",
    "            writer.writerow(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate using precision, recall and f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# evaluate\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate(gold, predictions):\n",
    "    class_report = classification_report(gold, predictions, labels = ['OFF', 'NOT'])\n",
    "    print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_count(path_to_data):\n",
    "    \n",
    "    train_data = load_data(data_dir, 'train')\n",
    "    test_data = load_data(data_dir, 'test')\n",
    "    train_data, val_data = split_train(train_data)\n",
    "    \n",
    "    tokenize(train_data, remove_stop_words = True)\n",
    "    tokenize(test_data, remove_stop_words = True)\n",
    "    tokenize(val_data, remove_stop_words = True)\n",
    "    \n",
    "    train_X, val_X = tweets_to_count_vec(train_data['tweet_tok'], val_data['tweet_tok']) \n",
    "    train_X, test_X = tweets_to_count_vec(train_data['tweet_tok'], test_data['tweet_tok'])\n",
    "    \n",
    "    train_y = train_data['subtask_a']\n",
    "    \n",
    "    clf = train(train_X, train_y)\n",
    "    predictions_val = predict(clf, val_X)  \n",
    "    predictions_test = predict(clf, test_X)\n",
    "    \n",
    "    name_val = 'count_svm_val'\n",
    "    predictions_to_file(val_data['tweet'], val_data['subtask_a'], predictions_val, name_val)\n",
    "    name_test = 'count_svm_test'\n",
    "    predictions_to_file(test_data['tweet'], test_data['subtask_a'], predictions_test, name_test)\n",
    "    \n",
    "    print('--- performance on the validation set')\n",
    "    evaluate(val_data['subtask_a'], predictions_val)\n",
    "    print('--- performance on the test set')\n",
    "    evaluate(test_data['subtask_a'], predictions_test)\n",
    "    \n",
    "    print(f'valdidation predictions written to: predictions/{name_val}.csv')\n",
    "    print(f'test predictions written to: predictions/{name_test}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_embeddings(path_to_data, path_to_model, model_name):\n",
    "    \n",
    "    train_data = load_data(data_dir, 'train')\n",
    "    test_data = load_data(data_dir, 'test')\n",
    "    train_data, val_data = split_train(train_data)\n",
    "    \n",
    "    tokenize(train_data, remove_stop_words = True)\n",
    "    tokenize(test_data, remove_stop_words = True)\n",
    "    tokenize(val_data, remove_stop_words = True)\n",
    "    \n",
    "    train_X = tweets_to_embedding(train_data['tweet_tok'], path_to_model) \n",
    "    val_X = tweets_to_embedding(val_data['tweet_tok'], path_to_model)   \n",
    "    test_X = tweets_to_embedding(test_data['tweet_tok'], path_to_model) \n",
    "    \n",
    "    train_y = train_data['subtask_a']\n",
    "    \n",
    "    clf = train(train_X, train_y)\n",
    "    predictions_val = predict(clf, val_X)\n",
    "    predictions_test = predict(clf, test_X)\n",
    "    \n",
    "    name_val = f'embeddings-{model_name}_svm_val'\n",
    "    predictions_to_file(val_data['tweet'], val_data['subtask_a'], predictions_val, name_val)\n",
    "    name_test = f'embeddings-{model_name}_svm_test'\n",
    "    predictions_to_file(test_data['tweet'], test_data['subtask_a'], predictions_test, name_test)\n",
    "    \n",
    "    print('--- performance on the validation set')\n",
    "    evaluate(val_data['subtask_a'], predictions_val)\n",
    "    print('--- performance on the test set')\n",
    "    evaluate(test_data['subtask_a'], predictions_test)\n",
    "    \n",
    "    print(f'valdidation predictions written to: predictions/{name_val}.csv')\n",
    "    print(f'test predictions written to: predictions/{name_test}.csv')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run your experiments here\n",
    "\n",
    "Examples are given below. Note that you have to make sure the data are stored on your computer and that you have to modify the filepaths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../../../Data/offenseval/offenseval2017/offenseval-training-v1.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../../../../Data/offenseval/offenseval2017/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m classify_count(data_dir)\n",
      "Cell \u001b[1;32mIn [8], line 3\u001b[0m, in \u001b[0;36mclassify_count\u001b[1;34m(path_to_data)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclassify_count\u001b[39m(path_to_data):\n\u001b[1;32m----> 3\u001b[0m     train_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     test_data \u001b[38;5;241m=\u001b[39m load_data(data_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m     train_data, val_data \u001b[38;5;241m=\u001b[39m split_train(train_data)\n",
      "Cell \u001b[1;32mIn [1], line 16\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(data_dir, setname)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m setname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     15\u001b[0m     filepath \u001b[38;5;241m=\u001b[39m data_dir\u001b[38;5;241m+\u001b[39mtrain_path\n\u001b[1;32m---> 16\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\amris\\Documents\\school\\Subjectivity Mining\\A2\\ma-course-subjectivity-mining\\pynlp\\submining\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\amris\\Documents\\school\\Subjectivity Mining\\A2\\ma-course-subjectivity-mining\\pynlp\\submining\\lib\\site-packages\\pandas\\util\\_decorators.py:317\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    312\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    313\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    314\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    315\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(inspect\u001b[39m.\u001b[39mcurrentframe()),\n\u001b[0;32m    316\u001b[0m     )\n\u001b[1;32m--> 317\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\amris\\Documents\\school\\Subjectivity Mining\\A2\\ma-course-subjectivity-mining\\pynlp\\submining\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\amris\\Documents\\school\\Subjectivity Mining\\A2\\ma-course-subjectivity-mining\\pynlp\\submining\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\amris\\Documents\\school\\Subjectivity Mining\\A2\\ma-course-subjectivity-mining\\pynlp\\submining\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\amris\\Documents\\school\\Subjectivity Mining\\A2\\ma-course-subjectivity-mining\\pynlp\\submining\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1729\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1727\u001b[0m     is_text \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1728\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1729\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1730\u001b[0m     f,\n\u001b[0;32m   1731\u001b[0m     mode,\n\u001b[0;32m   1732\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1733\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1734\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1735\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1736\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1737\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1738\u001b[0m )\n\u001b[0;32m   1739\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1740\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\amris\\Documents\\school\\Subjectivity Mining\\A2\\ma-course-subjectivity-mining\\pynlp\\submining\\lib\\site-packages\\pandas\\io\\common.py:857\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    852\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    853\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    855\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    856\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    858\u001b[0m             handle,\n\u001b[0;32m    859\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    860\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    861\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    862\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    863\u001b[0m         )\n\u001b[0;32m    864\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    865\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    866\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../../../Data/offenseval/offenseval2017/offenseval-training-v1.tsv'"
     ]
    }
   ],
   "source": [
    "\n",
    "data_dir = '../../../../../Data/offenseval/offenseval2017/'\n",
    "classify_count(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "/anaconda/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "/anaconda/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- performance on the validation set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         OFF       0.76      0.51      0.61       440\n",
      "         NOT       0.79      0.92      0.85       884\n",
      "\n",
      "    accuracy                           0.78      1324\n",
      "   macro avg       0.77      0.72      0.73      1324\n",
      "weighted avg       0.78      0.78      0.77      1324\n",
      "\n",
      "--- performance on the test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         OFF       0.65      0.66      0.66        77\n",
      "         NOT       0.89      0.89      0.89       243\n",
      "\n",
      "    accuracy                           0.83       320\n",
      "   macro avg       0.77      0.78      0.77       320\n",
      "weighted avg       0.84      0.83      0.83       320\n",
      "\n",
      "valdidation predictions written to: predictions/embeddings-google_news_svm_val.csv\n",
      "test predictions written to: predictions/embeddings-google_news_svm_test.csv\n"
     ]
    }
   ],
   "source": [
    "path_to_model = '../../../../../Data/dsm/word2vec/GoogleNews-vectors-negative300.bin' \n",
    "data_dir = '../../../../../Data/offenseval/offenseval2017/'\n",
    "model_name = 'google_news'\n",
    "\n",
    "classify_embeddings(data_dir, path_to_model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('submining': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "b12ea830bc6de28409cef2ba5f37ea4e8f5bd63465e5fe37dfcb3d1c1e11965c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
